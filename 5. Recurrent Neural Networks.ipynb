{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "Classical neural networks, including convolutional ones, suffer from two severe limitations:\n",
    "\n",
    "+ They only accept a fixed-sized vector as input and produce a fixed-sized vector as output.\n",
    "+ They do not consider the sequential nature of some data (language, video frames, time series, etc.)\n",
    "\n",
    "Recurrent neural networks overcome these limitations by allowing to operate over sequences of vectors (in the input, in the output, or both).\n",
    "\n",
    "RNN can be interpreted as running a fixed program (consisting of a recurrent transformation that can be applied as many times as we like) with certain inputs and certain internal variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning to add\n",
    "\n",
    "Source: http://projects.rajivshah.com/blog/2016/04/05/rnn_addition/ \n",
    "\n",
    "The objective of this code developed by Rajiv Shah is to train a RNN for adding a sequence of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import basic libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn_cell\n",
    "from tensorflow.python.ops import rnn\n",
    "from tensorflow.python.ops import seq2seq\n",
    "from numpy import sum\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import *\n",
    "%matplotlib inline  \n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define first a set of hyperparameters, being the most important ``num_units``, that is the parameter that represents the internal memory in the basic LSTM cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_units = 50  \n",
    "input_size = 1      \n",
    "batch_size = 50    \n",
    "seq_len = 7\n",
    "drop_out = 0.6 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can write an auxiliar function to generate random sequences of integers (and the result of their addition):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[[0],\n",
      "        [4],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]]]), array([ 4.]))\n"
     ]
    }
   ],
   "source": [
    "# Creates our random sequences\n",
    "def gen_data(min_length=5, max_length=15, n_batch=50):\n",
    "\n",
    "    X = np.concatenate([np.random.randint(10,size=(n_batch, max_length, 1))],\n",
    "                       axis=-1)\n",
    "    y = np.zeros((n_batch,))\n",
    "    # Compute masks and correct values\n",
    "    for n in range(n_batch):\n",
    "        # Randomly choose the sequence length\n",
    "        length = np.random.randint(min_length, max_length)\n",
    "        X[n, length:, 0] = 0\n",
    "        # Sum the dimensions of X to get the target value\n",
    "        y[n] = np.sum(X[n, :, 0]*1)\n",
    "    return (X,y)\n",
    "\n",
    "print gen_data(2,5,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to star the model construction phase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "num_layers = 2\n",
    "cell = rnn_cell.BasicLSTMCell(num_units)\n",
    "cell = rnn_cell.MultiRNNCell([cell] * num_layers)\n",
    "cell = rnn_cell.DropoutWrapper(cell,output_keep_prob=drop_out)\n",
    "\n",
    "# Create placeholders for X and y\n",
    "inputs = [tf.placeholder(tf.float32,shape=[batch_size,1]) for _ in range(seq_len)]\n",
    "result = tf.placeholder(tf.float32, shape=[batch_size])\n",
    "\n",
    "# We initialize the initial cell state to 0\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "outputs, states = seq2seq.rnn_decoder(inputs, initial_state, cell, scope ='rnnln')\n",
    "\n",
    "# We are only interested in the final LSTM output value\n",
    "outputs2 = outputs[-1]\n",
    "\n",
    "# Tranformation of the final LSTM output value to a real value\n",
    "W_o = tf.Variable(tf.random_normal([num_units,input_size], stddev=0.01))     \n",
    "b_o = tf.Variable(tf.random_normal([input_size], stddev=0.01))\n",
    "outputs3 = tf.matmul(outputs2, W_o) + b_o\n",
    "\n",
    "# Definition of the mean square loss function\n",
    "cost = tf.pow(tf.sub(tf.reshape(outputs3, [-1]), result),2)\n",
    "train_op = tf.train.RMSPropOptimizer(0.005, 0.2).minimize(cost) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Generate Validation Data\n",
    "tempX,y_val = gen_data(5,seq_len,batch_size)\n",
    "X_val = []\n",
    "for i in range(seq_len):\n",
    "    X_val.append(tempX[:,i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "train_score =[]\n",
    "val_score= []\n",
    "x_axis=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9999/9999 [02:50<00:00, 58.79it/s]\n"
     ]
    }
   ],
   "source": [
    "num_epochs=10000\n",
    " \n",
    "for k in tqdm(range(1,num_epochs)):\n",
    "\n",
    "    #Generate Data for each epoch\n",
    "    tempX,y = gen_data(5,seq_len,batch_size)\n",
    "    X = []\n",
    "    for i in range(seq_len):\n",
    "        X.append(tempX[:,i,:])\n",
    "\n",
    "    #Create the dictionary of inputs to feed into sess.run\n",
    "    temp_dict = {inputs[i]:X[i] for i in range(seq_len)}\n",
    "    temp_dict.update({result: y})\n",
    "\n",
    "    _,c_train = sess.run([train_op,cost],feed_dict=temp_dict)   #perform an update on the parameters\n",
    "\n",
    "    val_dict = {inputs[i]:X_val[i] for i in range(seq_len)}  #create validation dictionary\n",
    "    val_dict.update({result: y_val})\n",
    "    c_val = sess.run([cost],feed_dict = val_dict )            #compute the cost on the validation set\n",
    "    if (k%100==0):\n",
    "        train_score.append(sum(c_train))\n",
    "        val_score.append(sum(c_val))\n",
    "        x_axis.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Train cost: 222.095123291, on Epoch 9999\n",
      "Final Validation cost: 229.60244751, on Epoch 9999\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhsAAAFkCAYAAACJu/k0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112842fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print \"Final Train cost: {}, on Epoch {}\".format(train_score[-1],k)\n",
    "print \"Final Validation cost: {}, on Epoch {}\".format(val_score[-1],k)\n",
    "plt.plot(train_score, 'r-', val_score, 'b-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##This part generates a new validation set to test against\n",
    "val_score_v =[]\n",
    "num_epochs=1\n",
    "\n",
    "for k in range(num_epochs):\n",
    "\n",
    "    #Generate Data for each epoch\n",
    "    tempX,y = gen_data(5,seq_len,batch_size)\n",
    "    X = []\n",
    "    for i in range(seq_len):\n",
    "        X.append(tempX[:,i,:])\n",
    "\n",
    "    val_dict = {inputs[i]:X[i] for i in range(seq_len)}\n",
    "    val_dict.update({result: y})\n",
    "    outv, c_val = sess.run([outputs3,cost],feed_dict = val_dict ) \n",
    "    val_score_v.append([c_val])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[4],\n",
       "        [4],\n",
       "        [0],\n",
       "        [3],\n",
       "        [5],\n",
       "        [6],\n",
       "        [0]]), 22.0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Target\n",
    "tempX[3],y[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 21.57432556], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Prediction\n",
    "outv[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "A Recurrent Neural Network (LSTM) implementation example using TensorFlow library.\n",
    "\n",
    "This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)\n",
    "Long Short Term Memory paper: http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf\n",
    "\n",
    "Author: Aymeric Damien\n",
    "Project: https://github.com/aymericdamien/TensorFlow-Examples/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "import numpy as np\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify images using a recurrent neural network, we consider every image row as a sequence of pixels. Because MNIST image shape is $28 \\times 28$ px, we will then handle 28 sequences of 28 steps for every sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 100000\n",
    "batch_size = 128\n",
    "\n",
    "display_step = 50\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 28 # number of sequences for every sample\n",
    "n_steps = 28 # number of timesteps for every sequence\n",
    "n_hidden = 64 # hidden layer num of features\n",
    "n_classes = 10 # total classes (0-9 digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input])\n",
    "\n",
    "# Tensorflow LSTM cell requires 2x n_hidden length (state & cell)\n",
    "istate = tf.placeholder(\"float\", [None, 2*n_hidden])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_input, n_hidden])), \n",
    "    # Hidden layer weights\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_hidden])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, n_steps, n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "    \n",
    "    # Permuting batch_size and n_steps\n",
    "    x = tf.transpose(x, [1, 0, 2])\n",
    "    # Reshaping to (n_steps*batch_size, n_input)\n",
    "    x = tf.reshape(x, [-1, n_input])\n",
    "    # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.split(0, n_steps, x)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "pred = RNN(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 6400, Minibatch Loss= 1.029582, Training Accuracy= 0.70312\n",
      "Iter 12800, Minibatch Loss= 0.575474, Training Accuracy= 0.80469\n",
      "Iter 19200, Minibatch Loss= 0.328122, Training Accuracy= 0.90625\n",
      "Iter 25600, Minibatch Loss= 0.369917, Training Accuracy= 0.88281\n",
      "Iter 32000, Minibatch Loss= 0.159932, Training Accuracy= 0.96094\n",
      "Iter 38400, Minibatch Loss= 0.277152, Training Accuracy= 0.90625\n",
      "Iter 44800, Minibatch Loss= 0.398959, Training Accuracy= 0.89844\n",
      "Iter 51200, Minibatch Loss= 0.182146, Training Accuracy= 0.96094\n",
      "Iter 57600, Minibatch Loss= 0.198010, Training Accuracy= 0.92969\n",
      "Iter 64000, Minibatch Loss= 0.156422, Training Accuracy= 0.96094\n",
      "Iter 70400, Minibatch Loss= 0.171092, Training Accuracy= 0.95312\n",
      "Iter 76800, Minibatch Loss= 0.147238, Training Accuracy= 0.95312\n",
      "Iter 83200, Minibatch Loss= 0.221436, Training Accuracy= 0.93750\n",
      "Iter 89600, Minibatch Loss= 0.088466, Training Accuracy= 0.96875\n",
      "Iter 96000, Minibatch Loss= 0.198219, Training Accuracy= 0.94531\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Reshape data to get 28 seq of 28 elements\n",
    "        batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch accuracy\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})\n",
    "            # Calculate batch loss\n",
    "            loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n",
    "            print \"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc)\n",
    "        step += 1\n",
    "    print \"Optimization Finished!\"\n",
    "\n",
    "    # Calculate accuracy for 128 mnist test images\n",
    "    test_len = 128\n",
    "    test_data = mnist.test.images[:test_len].reshape((-1, n_steps, n_input))\n",
    "    test_label = mnist.test.labels[:test_len]\n",
    "    print \"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_data, y: test_label})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 100000\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 28 # MNIST data input (img shape: 28*28)\n",
    "n_steps = 28 # timesteps\n",
    "n_hidden = 128 # hidden layer num of features\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    # Hidden layer weights => 2*n_hidden because of foward + backward cells\n",
    "    'out': tf.Variable(tf.random_normal([2*n_hidden, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BiRNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `bidirectional_rnn` function requirements\n",
    "    # Current data input shape: (batch_size, n_steps, n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "    \n",
    "    # Permuting batch_size and n_steps\n",
    "    x = tf.transpose(x, [1, 0, 2])\n",
    "    # Reshape to (n_steps*batch_size, n_input)\n",
    "    x = tf.reshape(x, [-1, n_input])\n",
    "    # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.split(0, n_steps, x)\n",
    "\n",
    "    # Define lstm cells with tensorflow\n",
    "    # Forward direction cell\n",
    "    lstm_fw_cell = rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "    # Backward direction cell\n",
    "    lstm_bw_cell = rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    try:\n",
    "        outputs, _, _ = rnn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n",
    "                                              dtype=tf.float32)\n",
    "    except Exception: # Old TensorFlow version only returns outputs not states\n",
    "        outputs = rnn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n",
    "                                        dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1280, Minibatch Loss= 1.860956, Training Accuracy= 0.33594\n",
      "Iter 2560, Minibatch Loss= 1.668990, Training Accuracy= 0.42188\n",
      "Iter 3840, Minibatch Loss= 1.267687, Training Accuracy= 0.52344\n",
      "Iter 5120, Minibatch Loss= 0.979197, Training Accuracy= 0.66406\n",
      "Iter 6400, Minibatch Loss= 0.840010, Training Accuracy= 0.69531\n",
      "Iter 7680, Minibatch Loss= 1.150313, Training Accuracy= 0.62500\n",
      "Iter 8960, Minibatch Loss= 0.872970, Training Accuracy= 0.72656\n",
      "Iter 10240, Minibatch Loss= 0.687948, Training Accuracy= 0.78125\n",
      "Iter 11520, Minibatch Loss= 0.468093, Training Accuracy= 0.89062\n",
      "Iter 12800, Minibatch Loss= 0.747362, Training Accuracy= 0.75781\n",
      "Iter 14080, Minibatch Loss= 0.563014, Training Accuracy= 0.81250\n",
      "Iter 15360, Minibatch Loss= 0.393497, Training Accuracy= 0.85938\n",
      "Iter 16640, Minibatch Loss= 0.453522, Training Accuracy= 0.88281\n",
      "Iter 17920, Minibatch Loss= 0.321291, Training Accuracy= 0.87500\n",
      "Iter 19200, Minibatch Loss= 0.237412, Training Accuracy= 0.94531\n",
      "Iter 20480, Minibatch Loss= 0.163636, Training Accuracy= 0.95312\n",
      "Iter 21760, Minibatch Loss= 0.442255, Training Accuracy= 0.82812\n",
      "Iter 23040, Minibatch Loss= 0.152966, Training Accuracy= 0.96094\n",
      "Iter 24320, Minibatch Loss= 0.372301, Training Accuracy= 0.89062\n",
      "Iter 25600, Minibatch Loss= 0.381968, Training Accuracy= 0.89062\n",
      "Iter 26880, Minibatch Loss= 0.247203, Training Accuracy= 0.91406\n",
      "Iter 28160, Minibatch Loss= 0.310479, Training Accuracy= 0.92188\n",
      "Iter 29440, Minibatch Loss= 0.248473, Training Accuracy= 0.91406\n",
      "Iter 30720, Minibatch Loss= 0.316642, Training Accuracy= 0.89062\n",
      "Iter 32000, Minibatch Loss= 0.209497, Training Accuracy= 0.92188\n",
      "Iter 33280, Minibatch Loss= 0.271357, Training Accuracy= 0.90625\n",
      "Iter 34560, Minibatch Loss= 0.182222, Training Accuracy= 0.94531\n",
      "Iter 35840, Minibatch Loss= 0.175153, Training Accuracy= 0.93750\n",
      "Iter 37120, Minibatch Loss= 0.267981, Training Accuracy= 0.91406\n",
      "Iter 38400, Minibatch Loss= 0.110685, Training Accuracy= 0.95312\n",
      "Iter 39680, Minibatch Loss= 0.140322, Training Accuracy= 0.95312\n",
      "Iter 40960, Minibatch Loss= 0.267737, Training Accuracy= 0.90625\n",
      "Iter 42240, Minibatch Loss= 0.162486, Training Accuracy= 0.94531\n",
      "Iter 43520, Minibatch Loss= 0.159897, Training Accuracy= 0.93750\n",
      "Iter 44800, Minibatch Loss= 0.208090, Training Accuracy= 0.91406\n",
      "Iter 46080, Minibatch Loss= 0.136048, Training Accuracy= 0.95312\n",
      "Iter 47360, Minibatch Loss= 0.213246, Training Accuracy= 0.93750\n",
      "Iter 48640, Minibatch Loss= 0.219253, Training Accuracy= 0.93750\n",
      "Iter 49920, Minibatch Loss= 0.204296, Training Accuracy= 0.95312\n",
      "Iter 51200, Minibatch Loss= 0.096653, Training Accuracy= 0.96094\n",
      "Iter 52480, Minibatch Loss= 0.145366, Training Accuracy= 0.94531\n",
      "Iter 53760, Minibatch Loss= 0.037344, Training Accuracy= 0.98438\n",
      "Iter 55040, Minibatch Loss= 0.095619, Training Accuracy= 0.97656\n",
      "Iter 56320, Minibatch Loss= 0.122473, Training Accuracy= 0.95312\n",
      "Iter 57600, Minibatch Loss= 0.157039, Training Accuracy= 0.94531\n",
      "Iter 58880, Minibatch Loss= 0.119124, Training Accuracy= 0.96875\n",
      "Iter 60160, Minibatch Loss= 0.097330, Training Accuracy= 0.97656\n",
      "Iter 61440, Minibatch Loss= 0.082321, Training Accuracy= 0.99219\n",
      "Iter 62720, Minibatch Loss= 0.279919, Training Accuracy= 0.91406\n",
      "Iter 64000, Minibatch Loss= 0.091292, Training Accuracy= 0.96875\n",
      "Iter 65280, Minibatch Loss= 0.128711, Training Accuracy= 0.96094\n",
      "Iter 66560, Minibatch Loss= 0.094494, Training Accuracy= 0.96875\n",
      "Iter 67840, Minibatch Loss= 0.137297, Training Accuracy= 0.95312\n",
      "Iter 69120, Minibatch Loss= 0.096668, Training Accuracy= 0.97656\n",
      "Iter 70400, Minibatch Loss= 0.075906, Training Accuracy= 0.96875\n",
      "Iter 71680, Minibatch Loss= 0.108690, Training Accuracy= 0.96094\n",
      "Iter 72960, Minibatch Loss= 0.067610, Training Accuracy= 0.97656\n",
      "Iter 74240, Minibatch Loss= 0.079254, Training Accuracy= 0.96875\n",
      "Iter 75520, Minibatch Loss= 0.113436, Training Accuracy= 0.95312\n",
      "Iter 76800, Minibatch Loss= 0.164539, Training Accuracy= 0.95312\n",
      "Iter 78080, Minibatch Loss= 0.052462, Training Accuracy= 0.97656\n",
      "Iter 79360, Minibatch Loss= 0.128879, Training Accuracy= 0.96094\n",
      "Iter 80640, Minibatch Loss= 0.141050, Training Accuracy= 0.96875\n",
      "Iter 81920, Minibatch Loss= 0.085931, Training Accuracy= 0.96094\n",
      "Iter 83200, Minibatch Loss= 0.098193, Training Accuracy= 0.96875\n",
      "Iter 84480, Minibatch Loss= 0.107070, Training Accuracy= 0.95312\n",
      "Iter 85760, Minibatch Loss= 0.102836, Training Accuracy= 0.95312\n",
      "Iter 87040, Minibatch Loss= 0.142373, Training Accuracy= 0.95312\n",
      "Iter 88320, Minibatch Loss= 0.107676, Training Accuracy= 0.96875\n",
      "Iter 89600, Minibatch Loss= 0.084592, Training Accuracy= 0.96875\n",
      "Iter 90880, Minibatch Loss= 0.182229, Training Accuracy= 0.95312\n",
      "Iter 92160, Minibatch Loss= 0.132014, Training Accuracy= 0.95312\n",
      "Iter 93440, Minibatch Loss= 0.113425, Training Accuracy= 0.96094\n",
      "Iter 94720, Minibatch Loss= 0.043284, Training Accuracy= 0.99219\n",
      "Iter 96000, Minibatch Loss= 0.063622, Training Accuracy= 0.97656\n",
      "Iter 97280, Minibatch Loss= 0.047848, Training Accuracy= 0.98438\n",
      "Iter 98560, Minibatch Loss= 0.245514, Training Accuracy= 0.96094\n",
      "Iter 99840, Minibatch Loss= 0.018196, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.984375\n"
     ]
    }
   ],
   "source": [
    "pred = BiRNN(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Reshape data to get 28 seq of 28 elements\n",
    "        batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch accuracy\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})\n",
    "            # Calculate batch loss\n",
    "            loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n",
    "            print \"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc)\n",
    "        step += 1\n",
    "    print \"Optimization Finished!\"\n",
    "\n",
    "    # Calculate accuracy for 128 mnist test images\n",
    "    test_len = 128\n",
    "    test_data = mnist.test.images[:test_len].reshape((-1, n_steps, n_input))\n",
    "    test_label = mnist.test.labels[:test_len]\n",
    "    print \"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_data, y: test_label})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Names (tflearn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 16632  | total loss: \u001b[1m\u001b[32m0.89219\u001b[0m\u001b[0m\n",
      "| Adam | epoch: 001 | loss: 0.89219 | val_loss: 0.71742 -- iter: 21378/21378\n",
      "Training Step: 16632  | total loss: \u001b[1m\u001b[32m0.89219\u001b[0m\u001b[0m\n",
      "| Adam | epoch: 001 | loss: 0.89219 | val_loss: 0.71742 -- iter: 21378/21378\n",
      "--\n",
      "salut\n",
      "maria salvadora\n",
      "maria paula\n",
      "maria paralmi\n",
      "ma\n",
      "---------------------------------\n",
      "Run id: nombres\n",
      "Log directory: /tmp/tflearn_logs/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Passing a `GraphDef` to the SummaryWriter is deprecated. Pass a `Graph` object instead, such as `sess.graph`.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-85829fe244b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_sequence_from_textfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     m.fit(X, Y, validation_set=0.1, batch_size=128,\n\u001b[0;32m---> 87\u001b[0;31m           n_epoch=1, run_id='nombres')\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jordi/Anaconda/anaconda/lib/python2.7/site-packages/tflearn/models/generator.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_inputs, Y_targets, n_epoch, validation_set, show_metric, batch_size, shuffle, snapshot_epoch, snapshot_step, run_id)\u001b[0m\n\u001b[1;32m    167\u001b[0m                          \u001b[0mdprep_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdprep_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                          \u001b[0mdaug_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdaug_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m                          run_id=run_id)\n\u001b[0m\u001b[1;32m    170\u001b[0m         self.predictor = Evaluator([self.net],\n\u001b[1;32m    171\u001b[0m                                    session=self.trainer.session)\n",
      "\u001b[0;32m/Users/jordi/Anaconda/anaconda/lib/python2.7/site-packages/tflearn/helpers/trainer.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, feed_dicts, n_epoch, val_feed_dicts, show_metric, snapshot_step, snapshot_epoch, shuffle_all, dprep_dict, daug_dict, run_id)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             self.summ_writer = tf.train.SummaryWriter(\n\u001b[0;32m--> 195\u001b[0;31m                 self.tensorboard_dir + run_id, self.session.graph_def)\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0mfeed_dicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dicts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jordi/Anaconda/anaconda/lib/python2.7/site-packages/tensorflow/python/training/summary_io.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, logdir, graph, max_queue, flush_secs, graph_def)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mgraph_def\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m       \u001b[0;31m# Calling it with both graph and graph_def for backward compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_logdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jordi/Anaconda/anaconda/lib/python2.7/site-packages/tensorflow/python/training/summary_io.pyc\u001b[0m in \u001b[0;36madd_graph\u001b[0;34m(self, graph, global_step, graph_def)\u001b[0m\n\u001b[1;32m    239\u001b[0m                       \"or the deprecated `GraphDef`\")\n\u001b[1;32m    240\u001b[0m     \u001b[0;31m# Finally, add the graph_def to the summary writer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_graph_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_graph_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0madd_run_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jordi/Anaconda/anaconda/lib/python2.7/site-packages/tensorflow/python/training/summary_io.pyc\u001b[0m in \u001b[0;36m_add_graph_def\u001b[0;34m(self, graph_def, global_step)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_add_graph_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0mgraph_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevent_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwall_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jordi/Anaconda/anaconda/lib/python2.7/site-packages/google/protobuf/internal/python_message.pyc\u001b[0m in \u001b[0;36mSerializeToString\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1053\u001b[0m           'Message %s is missing required fields: %s' % (\n\u001b[1;32m   1054\u001b[0m           self.DESCRIPTOR.full_name, ','.join(self.FindInitializationErrors())))\n\u001b[0;32m-> 1055\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializePartialToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1056\u001b[0m   \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSerializeToString\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jordi/Anaconda/anaconda/lib/python2.7/site-packages/google/protobuf/internal/python_message.pyc\u001b[0m in \u001b[0;36mSerializePartialToString\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1062\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mSerializePartialToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_InternalSerialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1065\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m   \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializePartialToString\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSerializePartialToString\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jordi/Anaconda/anaconda/lib/python2.7/site-packages/google/protobuf/internal/python_message.pyc\u001b[0m in \u001b[0;36mInternalSerialize\u001b[0;34m(self, write_bytes)\u001b[0m\n\u001b[1;32m   1068\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mInternalSerialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfield_descriptor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mListFields\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1070\u001b[0;31m       \u001b[0mfield_descriptor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1071\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtag_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_bytes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unknown_fields\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m       \u001b[0mwrite_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jordi/Anaconda/anaconda/lib/python2.7/site-packages/google/protobuf/internal/encoder.pyc\u001b[0m in \u001b[0;36mEncodeRepeatedField\u001b[0;34m(write, value)\u001b[0m\n\u001b[1;32m    754\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m         \u001b[0mlocal_EncodeVarint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m         \u001b[0melement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_InternalSerialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mEncodeRepeatedField\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jordi/Anaconda/anaconda/lib/python2.7/site-packages/google/protobuf/internal/python_message.pyc\u001b[0m in \u001b[0;36mByteSize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfield_descriptor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mListFields\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m       \u001b[0msize\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfield_descriptor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtag_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_bytes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unknown_fields\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jordi/Anaconda/anaconda/lib/python2.7/site-packages/google/protobuf/internal/encoder.pyc\u001b[0m in \u001b[0;36mFieldSize\u001b[0;34m(map_value)\u001b[0m\n\u001b[1;32m    357\u001b[0m       \u001b[0;31m# obvious way to avoid this within the current design without tons of code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m       \u001b[0;31m# duplication.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m       \u001b[0mentry_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concrete_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m       \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmessage_sizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jordi/Anaconda/anaconda/lib/python2.7/site-packages/google/protobuf/internal/python_message.pyc\u001b[0m in \u001b[0;36minit\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    474\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_byte_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_byte_size_dirty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tflearn, time, tensorflow\n",
    "\n",
    "tensorflow.reset_default_graph()\n",
    "\n",
    "def textfile_to_seq(file, seq_maxlen=25, redun_step=3):\n",
    "    \"\"\" string_to_semi_redundant_sequences.\n",
    "    Vectorize a string and returns parsed sequences and targets, along with\n",
    "    the associated dictionary.\n",
    "    Arguments:\n",
    "        string: `str`. Lower-case text from input text file.\n",
    "        seq_maxlen: `int`. Maximum length of a sequence. Default: 25.\n",
    "        redun_step: `int`. Redundancy step. Default: 3.\n",
    "    Returns:\n",
    "        `tuple`: (inputs, targets, dictionary)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import re\n",
    "    print(\"Vectorizing text...\")\n",
    "    \n",
    "    import codecs\n",
    "    f = codecs.open(file, \"r\", \"utf-8\")\n",
    "    string = f.read()\n",
    "    string.encode('utf-8')\n",
    "    string = string.lower()\n",
    "    chars = set()\n",
    "    chars.update(string)\n",
    "    char_idx = {c: i for i, c in enumerate(chars)}\n",
    "\n",
    "    sequences = []\n",
    "    next_chars = []\n",
    "    for i in range(0, len(string) - seq_maxlen, redun_step):\n",
    "        sequences.append(string[i: i + seq_maxlen])\n",
    "        next_chars.append(string[i + seq_maxlen])\n",
    "\n",
    "    X = np.zeros((len(sequences), seq_maxlen, len(chars)), dtype=np.bool)\n",
    "    Y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        for t, char in enumerate(seq):\n",
    "            X[i, t, char_idx[char]] = 1\n",
    "        Y[i, char_idx[next_chars[i]]] = 1\n",
    "\n",
    "    print(\"Text total length: \" + str(len(string)))\n",
    "    print(\"Distinct chars: \" + str(len(chars)))\n",
    "    print(\"Total sequences: \" + str(len(sequences)))\n",
    "    return X, Y, char_idx\n",
    "\n",
    "def random_sequence_from_string(string, seq_maxlen):\n",
    "    import random\n",
    "    rand_index = random.randint(0, len(string) - seq_maxlen - 1)\n",
    "    return string[rand_index: rand_index + seq_maxlen]\n",
    "\n",
    "def random_sequence_from_textfile(path, seq_maxlen):\n",
    "    import codecs\n",
    "    import re\n",
    "    f = codecs.open(path, \"r\", \"utf-8\")\n",
    "    text = f.read()\n",
    "    text.encode('utf-8')\n",
    "    text = text.lower()\n",
    "    return random_sequence_from_string(text, seq_maxlen)\n",
    "\n",
    "# path = 'toponims.txt'\n",
    "path = \"NombresMujerBarcelona.txt\"\n",
    "\n",
    "maxlen = 20\n",
    "\n",
    "X, Y, char_idx = \\\n",
    "    textfile_to_seq(path, seq_maxlen=maxlen, redun_step=2)\n",
    "\n",
    "g = tflearn.input_data(shape=[None, maxlen, len(char_idx)])\n",
    "g = tflearn.lstm(g, 64, return_seq=True)\n",
    "g = tflearn.dropout(g, 0.5)\n",
    "g = tflearn.lstm(g, 64)\n",
    "g = tflearn.dropout(g, 0.5)\n",
    "g = tflearn.fully_connected(g, len(char_idx), activation='softmax')\n",
    "g = tflearn.regression(g, optimizer='adam', loss='categorical_crossentropy',\n",
    "                       learning_rate=0.01)\n",
    "\n",
    "m = tflearn.SequenceGenerator(g, dictionary=char_idx,\n",
    "                              seq_maxlen=maxlen,\n",
    "                              clip_gradients=5.0)\n",
    "\n",
    "# names\n",
    "for i in range(100):\n",
    "    seed = random_sequence_from_textfile(path, maxlen)\n",
    "    m.fit(X, Y, validation_set=0.1, batch_size=128,\n",
    "          n_epoch=1, run_id='nombres')\n",
    "    print(m.generate(30, temperature=1.2, seq_seed=seed).encode('utf-8'))\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tflearn, time\n",
    "\n",
    "def textfile_to_seq(file, seq_maxlen=25, redun_step=3):\n",
    "    \"\"\" string_to_semi_redundant_sequences.\n",
    "    Vectorize a string and returns parsed sequences and targets, along with\n",
    "    the associated dictionary.\n",
    "    Arguments:\n",
    "        string: `str`. Lower-case text from input text file.\n",
    "        seq_maxlen: `int`. Maximum length of a sequence. Default: 25.\n",
    "        redun_step: `int`. Redundancy step. Default: 3.\n",
    "    Returns:\n",
    "        `tuple`: (inputs, targets, dictionary)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import re\n",
    "    print(\"Vectorizing text...\")\n",
    "    \n",
    "    import codecs\n",
    "    f = codecs.open(file, \"r\", \"utf-8\")\n",
    "    string = f.read()\n",
    "    string.encode('utf-8')\n",
    "    string = re.sub( '([A-Z])', '^\\\\1', string ).lower()\n",
    "    chars = set()\n",
    "    chars.update(string)\n",
    "    char_idx = {c: i for i, c in enumerate(chars)}\n",
    "\n",
    "    sequences = []\n",
    "    next_chars = []\n",
    "    for i in range(0, len(string) - seq_maxlen, redun_step):\n",
    "        sequences.append(string[i: i + seq_maxlen])\n",
    "        next_chars.append(string[i + seq_maxlen])\n",
    "\n",
    "    X = np.zeros((len(sequences), seq_maxlen, len(chars)), dtype=np.bool)\n",
    "    Y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        for t, char in enumerate(seq):\n",
    "            X[i, t, char_idx[char]] = 1\n",
    "        Y[i, char_idx[next_chars[i]]] = 1\n",
    "\n",
    "    print(\"Text total length: \" + str(len(string)))\n",
    "    print(\"Distinct chars: \" + str(len(chars)))\n",
    "    print(\"Total sequences: \" + str(len(sequences)))\n",
    "    return X, Y, char_idx\n",
    "\n",
    "def random_sequence_from_string(string, seq_maxlen):\n",
    "    import random\n",
    "    rand_index = random.randint(0, len(string) - seq_maxlen - 1)\n",
    "    return string[rand_index: rand_index + seq_maxlen]\n",
    "\n",
    "def random_sequence_from_textfile(path, seq_maxlen):\n",
    "    import codecs\n",
    "    import re\n",
    "    f = codecs.open(path, \"r\", \"utf-8\")\n",
    "    text = f.read()\n",
    "    text.encode('utf-8')\n",
    "    text = re.sub( '([A-Z])', '^\\\\1', text ).lower()\n",
    "    return random_sequence_from_string(text, seq_maxlen)\n",
    "\n",
    "path = 'toponims.txt'\n",
    "\n",
    "maxlen = 20\n",
    "\n",
    "X, Y, char_idx = \\\n",
    "    textfile_to_seq(path, seq_maxlen=maxlen, redun_step=2)\n",
    "\n",
    "g = tflearn.input_data(shape=[None, maxlen, len(char_idx)])\n",
    "g = tflearn.lstm(g, 64, return_seq=True)\n",
    "g = tflearn.dropout(g, 0.5)\n",
    "g = tflearn.lstm(g, 64)\n",
    "g = tflearn.dropout(g, 0.5)\n",
    "g = tflearn.fully_connected(g, len(char_idx), activation='softmax')\n",
    "g = tflearn.regression(g, optimizer='adam', loss='categorical_crossentropy',\n",
    "                       learning_rate=0.01)\n",
    "\n",
    "m = tflearn.SequenceGenerator(g, dictionary=char_idx,\n",
    "                              seq_maxlen=maxlen,\n",
    "                              clip_gradients=5.0)\n",
    "\n",
    "for i in range(10):\n",
    "    seed = random_sequence_from_textfile(path, maxlen)\n",
    "    m.fit(X, Y, validation_set=0.1, batch_size=128,\n",
    "          n_epoch=1, run_id='toponims')\n",
    "    f = open('names.txt','wb')\n",
    "    f.write(\"-- TESTING... \\n\")\n",
    "    f.write(\"-- EPOCH = \" + str(i) + \"\\n\")\n",
    "    f.write(\"-- Test with temperature of 1.2 -- \\n\")\n",
    "    f.write(m.generate(30, temperature=1.2, seq_seed=seed).encode('ascii','ignore')+ \"\\n\")\n",
    "    f.write(\"-- Test with temperature of 1.0 -- \\n\")\n",
    "    f.write(m.generate(30, temperature=1.0, seq_seed=seed).encode('ascii','ignore')+ \"\\n\")\n",
    "    f.write(\"-- Test with temperature of 0.5 -- \\n\")\n",
    "    f.write(m.generate(30, temperature=0.5, seq_seed=seed).encode('ascii','ignore')+ \"\\n\")\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
