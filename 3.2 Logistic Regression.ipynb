{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Logistic regression learning algorithm example using TensorFlow library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Logistic regression model is one of the simplest classification models. The most basic form deals with classifing a given set of data points into two possible classes, usually labelled as  **0**  and **1**. The logistic regression model thus predicts an output y in {**0**,**1**}, given an input vector **x**. The probability is modeled using the logistic function $$ g(z)=1/(1+e^{-z})$$Namely, the probability of finding the output **y=1** is given by\n",
    "$$ q_{{y=1}}\\ =\\ {\\hat  {y}}\\ \\equiv \\ g({\\mathbf  {w}}\\cdot {\\mathbf  {x}} + b)\\,,$$\n",
    "while the probability of finding **y=0** is given by\n",
    "$$ q_{{y=0}} = 1 - q_{{y=1}}$$\n",
    "\n",
    "Weights **w** are usually learned in the training step by using some optimization algorithem like gradient descent.\n",
    "\n",
    "The typical loss function that one uses in logistic regression is computed by taking the average of all cross-entropies in the sample. For example, suppose we have N￼ samples the loss function is then given by:\n",
    "$$L(w)\\frac{1}{N}\\sum_{n=1}^{N}H(p_{n},q_{n})=-{\\frac  1N}\\sum_{{n=1}}^{N}\\ {\\bigg [}y_{n}\\log {\\hat  y}_{n}+(1-y_{n})\\log(1-{\\hat  y}_{n}){\\bigg ]}$$\n",
    "\n",
    "In this example we will use MNIST database of handwritten digits provided in the tensorflow package. The corresponding labels in MNIST are numbers between 0 and 9, describing which digit a given image is. In order to deal with this problem we are going to use label representation of \"one-hot vectors\". A one-hot vector representation is a vector which is 0 in most dimensions, and 1 in a single dimension. In this case, the nth digit will be represented as a vector which is 1 in the nth dimensions. For example, 3 would be [0,0,0,1,0,0,0,0,0,0]. \n",
    "\n",
    "In the case of multiclass the output is given by:\n",
    "$$ \\hat{y} = softmax(g(w⋅x + b))$$\n",
    "which can be simplified by:\n",
    "$$ \\hat{y} = softmax(w⋅x + b)$$ \n",
    "and the loss is defined as:\n",
    "$$ L(w) = \\frac{1}{N}\\sum_{n=1}^{N}H(p_{n},q_{n})=-\\frac{1}{N}\\sum_{n=1}^{N}y_{n}log(\\hat{y}_{n})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import tensorflow\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# tf Graph Input\n",
    "X = tf.placeholder(\"float\", [None, 784]) # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(\"float\", [None, 10]) # 0-9 digits recognition => 10 classes\n",
    "\n",
    "# Create model\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# Construct model\n",
    "y_pred = tf.nn.softmax(tf.add(tf.matmul(X, W),b)) # Softmax  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Training Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 25\n",
    "batch_size = 100\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Minimize error using cross entropy\n",
    "# Cross entropy\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(y_pred), reduction_indices=1))\n",
    "\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 651.181941569\n",
      "Epoch: 0002 cost= 366.013635308\n",
      "Epoch: 0003 cost= 304.101509273\n",
      "Epoch: 0004 cost= 274.323226869\n",
      "Epoch: 0005 cost= 256.067038000\n",
      "Epoch: 0006 cost= 243.445543528\n",
      "Epoch: 0007 cost= 234.052417025\n",
      "Epoch: 0008 cost= 226.725508586\n",
      "Epoch: 0009 cost= 220.797414109\n",
      "Epoch: 0010 cost= 215.790569171\n",
      "Epoch: 0011 cost= 211.618999228\n",
      "Epoch: 0012 cost= 208.022925720\n",
      "Epoch: 0013 cost= 204.804209471\n",
      "Epoch: 0014 cost= 202.015058577\n",
      "Epoch: 0015 cost= 199.529044494\n",
      "Epoch: 0016 cost= 197.226270527\n",
      "Epoch: 0017 cost= 195.181027442\n",
      "Epoch: 0018 cost= 193.292408586\n",
      "Epoch: 0019 cost= 191.592877552\n",
      "Epoch: 0020 cost= 189.988113314\n",
      "Epoch: 0021 cost= 188.518636912\n",
      "Epoch: 0022 cost= 187.155902281\n",
      "Epoch: 0023 cost= 185.873152584\n",
      "Epoch: 0024 cost= 184.656975031\n",
      "Epoch: 0025 cost= 183.516896144\n",
      "Optimization Finished!\n",
      "Accuracy: 0.914\n"
     ]
    }
   ],
   "source": [
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Fit training using batch data\n",
    "            sess.run([optimizer, cost ]  feed_dict={X: batch_xs, y: batch_ys})\n",
    "            # Compute average loss\n",
    "            avg_cost += sess.run(cost, feed_dict={X: batch_xs, y: batch_ys})/total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost)\n",
    "\n",
    "    print \"Optimization Finished!\"\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print \"Accuracy:\", accuracy.eval({X: mnist.test.images, y: mnist.test.labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
