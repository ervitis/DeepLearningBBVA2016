{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Sentiment Analysis\n",
    "\n",
    "### IMDB Movie Review Sentiment Problem Description\n",
    "\n",
    "The dataset is the Large Movie Review Dataset often referred to as the IMDB dataset.\n",
    "\n",
    "The Large Movie Review Dataset (often referred to as the IMDB dataset) contains 25,000 highly polar moving reviews (good or bad) for training and the same amount again for testing. The problem is to determine whether a given moving review has a positive or negative sentiment.\n",
    "\n",
    "The data was collected by Stanford researchers and was used in a 2011 paper [PDF] where a split of 50/50 of the data was used for training and test. An accuracy of 88.89% was achieved.\n",
    "\n",
    "The data was also used as the basis for a Kaggle competition titled “Bag of Words Meets Bags of Popcorn” in late 2014 to early 2015. Accuracy was achieved above 97% with winners achieving 99%.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_full.pkl\n",
      "65511424/65552540 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from matplotlib import pyplot\n",
    "# load the dataset\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data()\n",
    "X = numpy.concatenate((X_train, X_test), axis=0)\n",
    "y = numpy.concatenate((y_train, y_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: \n",
      "(50000,)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "# summarize size\n",
    "print(\"Training data: \")\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: \n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "# Summarize number of classes\n",
    "print(\"Classes: \")\n",
    "print(numpy.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: \n",
      "88585\n"
     ]
    }
   ],
   "source": [
    "# Summarize number of words\n",
    "print(\"Number of words: \")\n",
    "print(len(numpy.unique(numpy.hstack(X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review length: \n",
      "Mean 234.76 words (172.911495)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAg0AAAFkCAYAAACjCwibAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5bc45fd510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summarize review length\n",
    "print(\"Review length: \")\n",
    "result = map(len, X)\n",
    "print(\"Mean %.2f words (%f)\" % (numpy.mean(result), numpy.std(result)))\n",
    "# plot review length\n",
    "pyplot.boxplot(result)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "A recent breakthrough in the field of natural language processing is called word embedding.\n",
    "\n",
    "This is a technique where words are encoded as real-valued vectors in a high dimensional space, where the similarity between words in terms of meaning translates to closeness in the vector space.\n",
    "\n",
    "Discrete words are mapped to vectors of continuous numbers. This is useful when working with natural language problems with neural networks and deep learning models are we require numbers as input.\n",
    "\n",
    "Keras provides a convenient way to convert positive integer representations of words into a word embedding by an Embedding layer.\n",
    "\n",
    "The layer takes arguments that define the mapping including the maximum number of expected words also called the vocabulary size (e.g. the largest integer value that will be seen as an integer). The layer also allows you to specify the dimensionality for each word vector, called the output dimension.\n",
    "\n",
    "We would like to use a word embedding representation for the IMDB dataset.\n",
    "\n",
    "Let’s say that we are only interested in the first 5,000 most used words in the dataset. Therefore our vocabulary size will be 5,000. We can choose to use a 32-dimension vector to represent each word. Finally, we may choose to cap the maximum review length at 500 words, truncating reviews longer than that and padding reviews shorter than that with 0 values.\n",
    "\n",
    "We would load the IMDB dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([ [1, 14, 22, 16, 43, 2, 2, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 2, 112, 50, 2, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 2, 17, 2, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 2, 38, 76, 15, 13, 2, 4, 22, 17, 2, 17, 12, 16, 2, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 2, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 2, 36, 71, 43, 2, 476, 26, 400, 317, 46, 7, 4, 2, 2, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32],\n",
       "         [1, 194, 2, 194, 2, 78, 228, 5, 6, 2, 2, 2, 134, 26, 4, 2, 8, 118, 2, 14, 394, 20, 13, 119, 2, 189, 102, 5, 207, 110, 2, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2, 2, 5, 2, 4, 116, 9, 35, 2, 4, 229, 9, 340, 2, 4, 118, 9, 4, 130, 2, 19, 4, 2, 5, 89, 29, 2, 46, 37, 4, 455, 9, 45, 43, 38, 2, 2, 398, 4, 2, 26, 2, 5, 163, 11, 2, 2, 4, 2, 9, 194, 2, 7, 2, 2, 349, 2, 148, 2, 2, 2, 15, 123, 125, 68, 2, 2, 15, 349, 165, 2, 98, 5, 4, 228, 9, 43, 2, 2, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 2, 228, 2, 5, 2, 2, 245, 2, 5, 4, 2, 131, 152, 491, 18, 2, 32, 2, 2, 14, 9, 6, 371, 78, 22, 2, 64, 2, 9, 8, 168, 145, 23, 4, 2, 15, 16, 4, 2, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95],\n",
       "         [1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 2, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2, 311, 12, 16, 2, 33, 75, 43, 2, 296, 4, 86, 320, 35, 2, 19, 263, 2, 2, 4, 2, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 2, 43, 2, 2, 8, 257, 85, 2, 42, 2, 2, 83, 68, 2, 15, 36, 165, 2, 278, 36, 69, 2, 2, 8, 106, 14, 2, 2, 18, 6, 22, 12, 215, 28, 2, 40, 6, 87, 326, 23, 2, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2, 51, 9, 170, 23, 2, 116, 2, 2, 13, 191, 79, 2, 89, 2, 14, 9, 8, 106, 2, 2, 35, 2, 6, 227, 7, 129, 113],\n",
       "         ...,\n",
       "         [1, 11, 6, 230, 245, 2, 9, 6, 2, 446, 2, 45, 2, 84, 2, 2, 21, 4, 2, 84, 2, 325, 2, 134, 2, 2, 84, 5, 36, 28, 57, 2, 21, 8, 140, 8, 2, 5, 2, 84, 56, 18, 2, 14, 9, 31, 7, 4, 2, 2, 2, 2, 2, 18, 6, 20, 207, 110, 2, 12, 8, 2, 2, 8, 97, 6, 20, 53, 2, 74, 4, 460, 364, 2, 29, 270, 11, 2, 108, 45, 40, 29, 2, 395, 11, 6, 2, 2, 7, 2, 89, 364, 70, 29, 140, 4, 64, 2, 11, 4, 2, 26, 178, 4, 2, 443, 2, 5, 27, 2, 117, 2, 2, 165, 47, 84, 37, 131, 2, 14, 2, 10, 10, 61, 2, 2, 10, 10, 288, 2, 2, 34, 2, 2, 4, 65, 496, 4, 231, 7, 2, 5, 6, 320, 234, 2, 234, 2, 2, 7, 496, 4, 139, 2, 2, 2, 2, 5, 2, 18, 4, 2, 2, 250, 11, 2, 2, 4, 2, 2, 2, 2, 372, 2, 2, 2, 2, 7, 4, 59, 2, 4, 2, 2],\n",
       "         [1, 2, 2, 69, 72, 2, 13, 2, 2, 8, 12, 2, 23, 5, 16, 484, 2, 54, 349, 11, 2, 2, 45, 58, 2, 13, 197, 12, 16, 43, 23, 2, 5, 62, 30, 145, 402, 11, 2, 51, 2, 32, 61, 369, 71, 66, 2, 12, 2, 75, 100, 2, 8, 4, 105, 37, 69, 147, 2, 75, 2, 44, 257, 390, 5, 69, 263, 2, 105, 50, 286, 2, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 2, 13, 2, 40, 319, 2, 112, 2, 11, 2, 121, 25, 70, 2, 4, 2, 2, 13, 18, 31, 62, 40, 8, 2, 4, 2, 7, 14, 123, 5, 2, 25, 8, 2, 12, 145, 5, 202, 12, 160, 2, 202, 12, 6, 52, 58, 2, 92, 401, 2, 12, 39, 14, 251, 8, 15, 251, 5, 2, 12, 38, 84, 80, 124, 12, 9, 23],\n",
       "         [1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 2, 270, 2, 5, 2, 2, 2, 2, 101, 405, 39, 14, 2, 4, 2, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 2, 102, 7, 4, 2, 2, 9, 24, 6, 78, 2, 17, 2, 2, 21, 27, 2, 2, 5, 2, 2, 92, 2, 4, 2, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 2, 2, 9, 6, 66, 78, 2, 4, 2, 2, 5, 2, 272, 191, 2, 6, 2, 8, 2, 2, 2, 2, 5, 383, 2, 2, 2, 2, 497, 2, 8, 2, 2, 2, 21, 60, 27, 239, 9, 43, 2, 209, 405, 10, 10, 12, 2, 40, 4, 248, 20, 12, 16, 5, 174, 2, 72, 7, 51, 6, 2, 22, 4, 204, 131, 9]], dtype=object),\n",
       "  array([1, 0, 0, ..., 0, 1, 0])),\n",
       " (array([ [1, 89, 27, 2, 2, 17, 199, 132, 5, 2, 16, 2, 24, 8, 2, 4, 2, 7, 4, 22, 2, 2, 16, 2, 17, 2, 7, 2, 2, 9, 4, 2, 8, 14, 2, 13, 2, 38, 19, 27, 239, 13, 100, 235, 61, 483, 2, 4, 7, 4, 20, 131, 2, 72, 8, 14, 251, 27, 2, 7, 308, 16, 2, 2, 17, 29, 144, 28, 77, 2, 18, 12],\n",
       "         [1, 2, 7, 2, 2, 2, 31, 314, 17, 2, 2, 2, 2, 2, 83, 4, 2, 2, 33, 27, 2, 2, 2, 32, 4, 189, 22, 11, 2, 2, 29, 2, 4, 2, 7, 4, 2, 2, 15, 2, 455, 2, 2, 2, 2, 96, 145, 11, 4, 204, 2, 297, 2, 29, 2, 4, 2, 8, 35, 2, 2, 121, 2, 2, 2, 2, 2, 2, 2, 2, 2, 304, 4, 2, 145, 8, 41, 2, 50, 2, 2, 2, 2, 2, 34, 2, 2, 145, 295, 174, 2, 6, 2, 18, 274, 2, 90, 145, 8, 2, 113, 155, 92, 140, 17, 2, 69, 2, 2, 2, 46, 24, 8, 30, 4, 132, 7, 41, 2, 103, 32, 38, 59, 2, 90, 11, 6, 297, 2, 33, 63, 2, 9, 329, 74, 2, 137, 2, 304, 6, 2, 2, 2, 2, 41, 2, 15, 274, 2, 41, 145, 8, 113, 11, 4, 2, 7, 6, 2, 2, 2, 17, 6, 2, 2, 181, 8, 30, 2, 11, 2, 2, 28, 8, 157, 295, 8, 79, 8, 6, 2, 11, 162, 2, 121, 2, 2, 2, 69, 77, 2, 19, 4, 2, 2, 8, 2, 68, 2, 145, 83, 406, 2, 4, 2, 7, 2, 2, 2, 2, 27, 2, 2, 2, 2, 37, 26, 199, 23, 4, 2, 39, 2, 2, 2, 7, 2, 2, 2, 308, 2, 80, 81, 2, 10, 10, 2, 34, 2, 2, 13, 119, 2, 7, 2, 4, 229, 34, 2, 2, 9, 87, 253, 55, 2, 2, 2, 441, 2, 2, 7, 85, 189, 22, 19, 52, 2, 39, 4, 2, 2, 121, 75, 67, 2, 2, 2, 2, 39, 4, 2, 4, 2, 108, 2, 2, 2, 2, 2, 39, 4, 6, 2, 23, 2, 2, 201, 488, 2, 2, 39, 4, 2, 2, 8, 4, 2, 343, 39, 2, 7, 2, 2, 54, 12, 2, 2, 4, 172, 136, 2, 7, 2, 115, 304, 410, 2, 63, 9, 43, 17, 73, 50, 26, 2, 7, 31, 2, 2, 2, 2, 15, 2, 2, 93, 2, 6, 171, 153, 2, 12, 152, 306, 2, 8, 2, 253, 33, 410, 4, 189, 2, 11, 2, 13, 119, 4, 136, 54, 2, 2, 26, 260, 6, 2, 2, 2, 2, 15, 2, 2, 29, 166, 163, 2, 2, 2, 469, 198, 24, 8, 135, 15, 50, 218, 6, 2, 52, 22, 11, 50, 17, 73, 88, 50, 91, 434, 9, 167, 2, 2, 8, 2, 52, 2, 6, 147, 281, 7, 253, 199, 406, 2, 2, 7, 105, 26, 2, 2, 17, 257, 2, 2, 68, 205, 2, 7, 2, 2, 15, 4, 2, 7, 2, 15, 36, 26, 2, 496, 62, 2, 2, 2, 2, 7, 2, 9, 87, 18, 4, 91, 173, 47, 15, 194, 352, 2, 44, 12, 33, 44, 2, 2, 2, 13, 144, 440, 38, 4, 64, 155, 15, 13, 80, 135, 9, 15, 49, 7, 4, 2, 302, 34, 2, 26, 6, 117, 2, 2, 13, 191, 377, 101, 2, 139, 11, 2, 7, 2, 345, 2, 4, 22, 152, 2, 4, 2, 2, 19, 6, 2, 2, 2, 2, 2, 83, 2, 393, 11, 2, 6, 2, 2, 2, 84, 2, 23, 2, 7, 2, 294, 112, 2, 34, 6, 2, 2, 6, 2, 125, 2, 2, 2, 2, 2, 4, 116, 9, 184, 52, 2, 17, 2, 9, 55, 163, 17, 29, 2, 4, 31, 2, 46, 13, 82, 40, 4, 139, 19, 2, 33, 4, 454, 169, 41, 55, 2, 54, 442, 2, 32, 15, 2, 2, 13, 191, 30, 4, 64, 31, 2, 13, 2, 104, 2, 7, 2, 9, 6, 2, 22, 2, 2, 39, 380, 8, 2, 87, 2, 189, 11, 2, 2, 33, 64, 2, 234, 196, 12, 115, 461, 357, 42, 2, 6, 2, 2, 7, 2, 106, 12, 17, 2, 17, 25, 70],\n",
       "         [1, 2, 256, 34, 31, 7, 4, 91, 2, 2, 7, 4, 236, 2, 7, 14, 2, 5, 82, 31, 7, 4, 91, 2, 2, 2, 2, 46, 7, 2, 59, 9, 389, 9, 175, 173, 15, 59, 299, 4, 2, 2, 9, 4, 2, 5, 2, 7, 4, 298, 438, 10, 10, 2, 2, 9, 2, 5, 41, 2, 2, 217, 73, 2, 34, 2, 284, 5, 82, 2, 2, 2, 2, 2, 2, 7, 4, 2, 255, 47, 6, 254, 58, 19, 4, 2, 2, 7, 27, 31, 283, 155, 5, 2, 27, 2, 339, 4, 338, 2, 2, 2, 2, 2, 2, 47, 96, 99, 76, 2, 7, 41, 57, 2, 4, 65, 304, 6, 55, 2, 2, 23, 4, 2, 7, 6, 2, 11, 14, 20, 4, 64, 2, 47, 8, 276, 41, 113, 23, 2, 8, 459, 18, 4, 2, 7, 409, 50, 9, 210, 31, 11, 175, 223, 37, 2, 15, 243, 7, 2, 2, 9, 2, 4, 454, 7, 4, 20, 21, 17, 58, 2, 59, 2, 56, 2, 41, 2, 113, 58, 2, 8, 41, 223, 59, 60, 2, 41, 2, 89, 81, 25, 81, 27, 175, 251, 11, 5, 46, 5, 2, 2, 12, 15, 9, 51, 372, 81, 6, 176, 7, 51, 13, 2, 2, 157, 2, 75, 2, 75, 2, 75, 2, 75, 2, 75, 2, 75, 26, 4, 118, 369, 75, 26, 4, 2, 2, 49, 7, 178, 40, 199, 372, 11, 14, 20, 28, 4, 404, 2, 26, 4, 2, 2, 18, 4, 436, 223, 5, 82, 81, 32, 15, 2, 157, 15, 9, 2, 2, 5, 111, 372, 11, 263, 2, 111, 7, 178, 28, 460, 2, 143, 15, 2, 7, 113, 54, 263, 2, 2, 5, 2, 13, 28, 77, 50, 36, 43, 435, 99, 185, 13, 28, 348, 61, 2, 61, 2, 21, 13, 115, 2, 98, 17, 73, 17, 54, 13, 69, 8, 297, 68, 2, 5, 69, 8, 2, 11, 68, 2, 14, 20, 2, 4, 2, 7, 113, 382, 12, 9, 2, 21, 15, 9, 89, 113, 9, 33, 211, 2, 6, 2, 33, 2, 9, 2, 415, 37, 2, 8, 104, 15, 27, 157, 9, 53, 2, 74, 2, 334, 5, 47, 6, 55, 2, 2, 2, 2, 4, 372, 11, 27, 113, 29, 9, 24, 2, 195, 8, 2, 48, 25, 181, 8, 67, 52, 116, 5, 4, 2, 7, 113, 81, 24, 2, 14, 20, 2, 139, 4, 2, 2, 8, 2, 2, 5, 32, 4, 231, 7, 6, 2, 46, 7, 2, 2, 15, 13, 38, 2, 75, 26, 32, 2, 2, 2, 2, 2, 12, 9, 64, 34, 170, 2, 15, 25, 2, 15, 25, 26, 66, 170, 2, 2, 25, 28, 6, 2, 2, 21, 121, 9, 129, 483, 10, 10],\n",
       "         ...,\n",
       "         [1, 14, 390, 7, 2, 2, 285, 4, 123, 9, 44, 8, 130, 45, 2, 2, 5, 32, 2, 9, 2, 2, 11, 14, 390, 4, 2, 2, 2, 35, 2, 2, 2, 2, 8, 4, 2, 4, 2, 90, 39, 4, 2, 2, 54, 2, 29, 2, 11, 17, 6, 2, 5, 95, 83, 27, 2, 2, 29, 2, 2, 6, 2, 63, 484, 2, 41, 46, 5, 2, 2, 41, 95, 2, 2, 2, 51, 9, 317, 7, 4, 2, 2, 266, 39, 4, 2, 5, 2, 4, 2, 2, 159, 385, 2, 4, 2, 21, 112, 4, 2, 7, 31, 12, 43, 2, 90, 2, 266, 8, 2, 4, 85, 2, 5, 494, 8, 169, 5, 2, 90, 18, 147, 2, 2, 9, 11, 4, 2, 269, 8, 169, 2, 54, 5, 2, 140, 46, 83, 4, 2, 8, 169, 4, 2, 4, 2, 2, 98, 103, 68, 2, 4, 2, 2, 15, 6, 370, 2, 285, 54, 36, 79, 145, 8, 2, 269, 8, 2, 4, 2, 5, 103, 36, 2, 6, 6, 2, 2, 2, 2, 2, 2, 41, 2, 8, 2, 84, 46, 7, 4, 96, 38, 59, 70, 79, 8, 4, 2, 21, 36, 79, 68, 8, 2, 5, 2, 2, 5, 43, 54, 9, 44, 8, 79, 324, 58, 9, 2, 8, 121, 36, 2, 2, 2, 8, 4, 2, 2, 11, 5, 2, 5, 2, 21, 2, 9, 131, 11, 4, 2, 38, 2, 4, 2, 5, 2, 46, 7, 4, 2, 54, 2, 417, 266, 29, 191, 2, 9, 351, 5, 38, 9, 4, 2, 7, 289, 18, 150, 2, 14, 390, 16, 2, 16, 4, 7, 32, 7, 98, 13, 62, 119, 8, 28, 41, 2, 7, 2, 13, 66, 92, 104, 2, 144, 7, 435, 8, 4, 2, 88, 48, 59, 161, 2, 28, 2, 21, 2, 2, 4, 2, 7, 289, 295, 174, 5, 146, 2, 19, 4, 2, 2],\n",
       "         [1, 13, 435, 83, 14, 22, 2, 2, 18, 6, 2, 2, 11, 405, 2, 7, 2, 2, 21, 51, 13, 188, 16, 53, 7, 6, 2, 2, 2, 19, 230, 99, 76, 2, 5, 24, 195, 206, 45, 2, 15, 14, 22, 16, 93, 23, 6, 352, 4, 2, 26, 2, 5, 2, 324, 137, 4, 116, 2, 6, 176, 8, 30, 2, 82, 4, 114, 2, 23, 6, 2, 7, 2, 6, 336, 5, 107, 2, 15, 2, 6, 2, 7, 2, 103, 2, 49, 2, 36, 216, 2, 6, 2, 2, 34, 6, 185, 250, 5, 41, 2, 5, 32, 14, 9, 2, 11, 2, 34, 4, 185, 250, 2, 2, 11, 35, 2, 45, 2, 15, 2, 2, 5, 2, 2, 197, 36, 71, 231, 142, 66, 2, 21, 466, 94, 118, 2, 2, 7, 2, 2, 9, 43, 99, 357, 8, 2, 4, 2, 4, 22, 2, 23, 18, 44, 2, 234, 5, 91, 7, 12, 2, 7, 357, 105, 2, 125, 357, 5, 196, 2, 414, 4, 64, 52, 155, 13, 28, 8, 135, 44, 4, 22, 9, 19, 2, 8, 4, 228, 63, 9, 52, 11, 2, 4, 277, 9, 4, 64, 85, 52, 155, 44, 4, 20, 5, 198, 64, 88, 45, 4, 236, 155, 15, 2, 13, 2, 386, 259, 2, 2, 14, 180, 50, 16, 76, 128, 2, 93, 11, 4, 2],\n",
       "         [1, 2, 54, 13, 435, 8, 67, 14, 20, 33, 4, 2, 2, 11, 2, 13, 122, 24, 2, 76, 13, 435, 8, 14, 20, 64, 88, 13, 2, 2, 45, 6, 2, 20, 2, 30, 52, 18, 6, 462, 95, 13, 2, 180, 5, 296, 12, 5, 219, 138, 36, 2, 2, 2, 2, 8, 297, 2, 2, 29, 9, 242, 31, 7, 4, 2, 493, 23, 4, 194, 268, 76, 433, 11, 61, 2, 74, 2, 42, 2, 5, 47, 31, 194, 2, 8, 85, 102, 15, 2, 72, 8, 6, 189, 20, 12, 287, 2, 2, 17, 294, 37, 9, 406, 29, 47, 6, 483, 57, 2, 89, 2, 5, 2, 12, 9, 29, 2, 2, 142, 15, 2, 115, 127, 42, 2, 8, 123, 29, 2, 2, 5, 2, 151, 174, 199, 7, 98, 2, 63, 25, 80, 2, 48, 25, 67, 4, 20, 32, 11, 32, 6, 275, 2, 11, 61, 2, 74, 111, 2, 5, 12, 2, 72, 11, 6, 171, 2, 17, 11, 37, 2, 11, 4, 130]], dtype=object),\n",
       "  array([1, 1, 1, ..., 1, 0, 1])))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb.load_data(nb_words=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Multi-Layer Perceptron Model for the IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MLP for the IMDB problem\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "test_split = 0.33\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_3 (Embedding)          (None, 500, 32)       160000      embedding_input_3[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)              (None, 16000)         0           embedding_3[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_19 (Dense)                 (None, 30)            480030      flatten_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_20 (Dense)                 (None, 30)            930         dense_19[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_21 (Dense)                 (None, 30)            930         dense_20[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_22 (Dense)                 (None, 1)             31          dense_21[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 641921\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "#model.add(Dense(256, input_dim=max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 12s - loss: 0.4832 - acc: 0.7336 - val_loss: 0.3191 - val_acc: 0.8634\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 14s - loss: 0.1912 - acc: 0.9282 - val_loss: 0.3332 - val_acc: 0.8674\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 11s - loss: 0.0657 - acc: 0.9812 - val_loss: 0.4192 - val_acc: 0.8626\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 12s - loss: 0.0141 - acc: 0.9971 - val_loss: 0.5548 - val_acc: 0.8620\n",
      "Accuracy: 86.20%\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=4, batch_size=128, verbose=1)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Dimensional Convolutional Neural Network Model for the IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CNN for the IMDB problem\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "test_split = 0.33\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=top_words)\n",
    "# pad dataset to a maximum review length in words\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_5 (Embedding)          (None, 500, 32)       160000      embedding_input_5[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_2 (Convolution1D)  (None, 500, 32)       3104        embedding_5[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_2 (MaxPooling1D)    (None, 250, 32)       0           convolution1d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)              (None, 8000)          0           maxpooling1d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_13 (Dense)                 (None, 250)           2000250     flatten_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_14 (Dense)                 (None, 1)             251         dense_13[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 2163605\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Convolution1D(nb_filter=32, filter_length=3, border_mode='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_length=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 239s - loss: 0.4257 - acc: 0.7768 - val_loss: 0.2825 - val_acc: 0.8815\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 243s - loss: 0.2172 - acc: 0.9153 - val_loss: 0.2813 - val_acc: 0.8839\n",
      "Accuracy: 88.39%\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=2, batch_size=128, verbose=1)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
